---
import CaseStudyTemplate from '../../components/CaseStudyTemplate.astro';

const data = {
  title: "Hybrid MuleSoft Platform: RTF on OpenShift",
  teaser: "Architected and delivered a secure, high-availability hybrid integration platform serving 5M+ members, leveraging MuleSoft Runtime Fabric (RTF) on OpenShift, F5 Big-IP, and enterprise GitLab CI/CD pipelines.",
  context: `The legacy infrastructure relied on monolithic ESB patterns, which lacked the elastic scalability needed for a rapidly growing ecosystem of 120+ microservices. Manual deployment processes and fragmented observability led to inconsistent MTTR and limited governance.

  The objective was to engineer a modern, hybrid platform combining the cost-efficiency of on-prem OpenShift with the orchestration power of MuleSoft Runtime Fabric (RTF), targeting 99.9% uptime and a significant reduction in deployment friction.`,
  mandate: `As the Technical Platform Lead, I was responsible for:
  - Designing the high-availability RTF on OpenShift architecture with F5 Big-IP for intelligent traffic management and TLS termination.
  - Engineering 100% automated GitLab CI/CD pipelines incorporating quality gates (MUnit), security scanning, and dynamic environment provisioning.
  - Establishing multi-region DR playbooks and automated secret rotation via AWS Secrets Manager.
  - Leading a cross-functional team of 15+ engineers through the migration of mission-critical member services.`,
  results: [
    "99.9% Uptime maintained during peak traffic for 5M+ members",
    "40% Reduction in deployment windows (from days to minutes)",
    "25% Operational efficiency gain via unified control plane",
    "Eliminated manual configuration errors through 100% Infrastructure-as-Code"
  ],
  differently: "In retrospect, I would have invested earlier in automated performance profiling within the CI/CD pipeline. While we hit our reliability targets, optimizing resource allocation on OpenShift became a manual overhead during peak traffic periods which could have been mitigated through automated load testing and predictive scaling.",
  diagram: `graph TD
    Client[External Clients / Mobile / Web] --> F5[F5 Big-IP Load Balancer]
    
    subgraph ControlPlane[MuleSoft Anypoint Platform]
        API_Manager[API Manager]
        Runtime_Manager[Runtime Manager]
    end

    subgraph AWS[AWS Region - CloudHub 2.0]
        CH2_Ingress[CloudHub 2.0 Ingress]
        Exp_API[Experience APIs]
        Sys_API_CH[External System APIs]
        CH2_Ingress --> Exp_API
        Exp_API --> Sys_API_CH
    end

    subgraph OnPrem[On-Premise Data Center]
        OpenShift[Red Hat OpenShift Cluster]
        subgraph RTF[Runtime Fabric]
            RTF_Ingress[RTF Ingress Controller]
            Process_API[Process / Orchestration APIs]
            Sys_API_OP[Internal System APIs]
            RTF_Ingress --> Process_API
            Process_API --> Sys_API_OP
        end
        Backends[(Legacy DB / ERP / Mainframe)]
        Sys_API_OP --> Backends
    end

    F5 -->|External Traffic| CH2_Ingress
    F5 -->|Internal/Secure Traffic| RTF_Ingress
    
    Sys_API_CH -.->|VPN/Direct Connect| RTF_Ingress
    
    ControlPlane -.->|Management| Exp_API
    ControlPlane -.->|Management| Process_API
    ControlPlane -.->|Management| Sys_API_CH
    ControlPlane -.->|Management| Sys_API_OP

    classDef aws fill:#fff7ed,stroke:#f97316,stroke-width:2px,color:#c2410c;
    classDef onprem fill:#fef2f2,stroke:#ef4444,stroke-width:2px,color:#b91c1c;
    classDef control fill:#eff6ff,stroke:#3b82f6,stroke-width:2px,color:#1d4ed8;
    classDef lb fill:#f0fdf4,stroke:#22c55e,stroke-width:2px,color:#15803d;
    
    class AWS aws;
    class OnPrem onprem;
    class ControlPlane control;
    class F5 lb;`
};
---

<CaseStudyTemplate {...data} />
